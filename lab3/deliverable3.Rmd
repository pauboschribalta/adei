---
title: "Entrega-2"
author: "Ivan Cala Mesa, Pau Bosch Ribalta"
date: "2023-03-28"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))

requiredPackages <- c("chemometrics","FactoMineR","car","knitr","missMDA","ResourceSelection", "pROC")
package.check <- lapply(requiredPackages, FUN = function(x) {
  for (x in requiredPackages) {
    if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies  = TRUE)
    library(x, character.only = TRUE)
  }
  }
})
```

```{r}
load("./bank-additional-clean.RData")
```

## Listing out variables

```{r}
summary(df)

# List of variable names for the target variable
target_vars <- names(df$y)

# List of variable names for the discrete variables
discrete_vars <- names(Filter(is.factor, df))
discrete_vars = discrete_vars[discrete_vars != 'y']

# List of variable names for the continuous variables
continuous_vars <- names(Filter(is.numeric, df))

```

Con el objetivo de poder encuadrar la variable binaria 'Y' en una regresión logística, será necesario asumir que las observaciones son independientes entre sí. Esto significa que los valores de la variable objetivo "Y" de una observación no están relacionados o influenciados por los valores de otras observaciones.

```{r}
# División del conjunto de datos en muestra de trabajo y muestra de prueba
set.seed(123)  # Para reproducibilidad
sample_size <- floor(0.8 * nrow(df))  # Tamaño de muestra de trabajo (80%)
train_index <- sample(seq_len(nrow(df)), size = sample_size)  # Índices de muestra de trabajo

# Creación de la muestra de trabajo y muestra de prueba
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
```

```{r}
continuous_vars
cor(df$duration, df[,continuous_vars[-1]])
```

When undertaking the analysis of various models in R, one may often confront a binomial regression problem. A binomial regression refers to the statistical process of modeling a binary outcome as a function of one or more predictors. One potential solution is to employ a linear model, which may be a favorable option under certain circumstances, particularly when elucidating the connection between predictor variables and the likelihood of a positive binary result.

The appeal of linear models for binomial regression problems resides in three major attributes:

1.  Coefficients Interpretation: Linear models facilitate the interpretation of the relationship between predictor variables and the expected outcome. They deliver coefficients that signify the direction and magnitude of the impact of various predictors on the probability of success.

2.  Simplicity and Familiarity: Linear regression has a long-standing history and broad application in statistics. This universal use and basic understanding make it an accessible tool for binomial regression, particularly when the focus is on discerning the linear connection between predictors and the log-odds of success.

3.  Continuous Predictor Variables: For binomial regression problems primarily featuring continuous predictors, a linear model can accurately portray their linear relationships with the log-odds of success.

However, it is essential to recognize that a linear model may not always be the optimal choice for binomial regression problems. Specifically designed for binary outcomes, alternative models may sometimes prove more suitable.

Indeed, binomial regression scenarios often necessitate models explicitly constructed for binary results. While linear models offer insights into the connection between predictors and the expected outcome, they may fail to adequately represent nonlinear relationships or consider the inherent probabilities associated with binary outcomes.

Alternative models tailored for binomial regression, such as logistic regression, probit regression, and complementary log-log regression, account for these underlying probabilities of success. They incorporate suitable link functions to model the relationship between predictors and the likelihood of success directly.

When analyzing our specific problem with 'y' as the target variable in R, fitting a linear model could help to illustrate the linear relationship between the predictor variables and the expected value of 'y'. Nonetheless, it remains crucial to reflect upon the assumptions and limitations of linear regression and investigate alternative models like logistic regression, which might more accurately model the probability of 'y' being a success.

```{r}
# Modelo inicial con dos variables numéricas y factores significativos
model <- glm(y ~ age_num + duration + job + housing + loan + month + euribor3m,
             data = train_data, family = binomial)
summary(model)
vif(model)
marginalModelPlots(model)
```

```{r}
Anova(model)
```

```{r}
model1 <- glm(y ~ duration + housing + month + euribor3m + marital + day_of_week,
             data = train_data, family = binomial)
summary(model1)
```

```{r}
vif(model1)
```

```{r}
marginalModelPlots(model1)
residualPlots(model1)
```

```{r}
Anova(model1)
vif(model1)
```

```{r}
# Incorporar interacciones entre factores y una covariable
model2 <- glm(y ~ duration * housing + euribor3m + marital * day_of_week + month,
                          data = train_data, family = binomial)

summary(model2)
Anova(model2)
vif(model2)
```

```{r}
marginalModelPlots(model2)
residualPlots(model2)
```

```{r}
# Incorporación de interacción entre factores
model3 <- glm(y ~ duration + housing*euribor3m + marital*day_of_week + month,
            data = train_data, family = binomial)
vif(model3)
```

```{r}
anova(model3)
Anova(model3)
```

```{r}
residualPlots(model3)
```

```{r}
summary(model3)
```

```{r}
# Crear una lista de los modelos
model_list <- list(model2, model3)
```

```{r}
# Crear una lista de etiquetas para los modelos
model_labels <- c("Model 2", "Model 3")

# Crear un layout de múltiples gráficos en una sola figura
par(mfrow = c(2, 2))  # 2 filas y 2 columnas para 4 gráficos

# Realizar los plots para cada modelo
lapply(seq_along(model_list), function(i) {
  plot(model_list[[i]], which = 1, main = paste("Residuos de Pearson -", model_labels[i]))
  plot(model_list[[i]], which = 2, main = paste("Residuos de Desviación -", model_labels[i]))
})

```

```{r}
# Model 2 Predictions and Confusion Matrix
model2_pred <- predict(model2, test_data, type = "response")
model2_pred_class <- ifelse(model2_pred > 0.5, "yes", "no")

# Model 2 Confusion Matrix
model2_actual <- test_data$y
model2_confusion <- table(Actual = model2_actual, Predicted = model2_pred_class)

# Model 3 Predictions and Confusion Matrix
model3_pred <- predict(model3, test_data, type = "response")
model3_pred_class <- ifelse(model3_pred > 0.5, "yes", "no")

# Model 3 Confusion Matrix
model3_actual <- test_data$y
model3_confusion <- table(Actual = model3_actual, Predicted = model3_pred_class)

model2_confusion
model3_confusion


```

```{r}
# Model 2 Accuracy and Recall
model2_accuracy <- sum(diag(model2_confusion)) / sum(model2_confusion)
model2_recall <- model2_confusion[2, 2] / sum(model2_confusion[2, ])

# Model 3 Accuracy and Recall
model3_accuracy <- sum(diag(model3_confusion)) / sum(model3_confusion)
model3_recall <- model3_confusion[2, 2] / sum(model3_confusion[2, ])

# Print the accuracy and recall for Model 2
cat("Model 2:\n")
cat("Accuracy:", model2_accuracy, "\n")
cat("Recall:", model2_recall, "\n")

# Print the accuracy and recall for Model 3
cat("Model 3:\n")
cat("Accuracy:", model3_accuracy, "\n")
cat("Recall:", model3_recall, "\n")

```

```{r}
# Predicción en la muestra de trabajo
train_pred <- predict(model3, train_data, type = "response")
train_pred_class <- ifelse(train_pred > 0.5, "yes", "no")

# Matriz de confusión en la muestra de trabajo
train_actual <- train_data$y
train_confusion <- table(Actual = train_actual, Predicted = train_pred_class)
train_confusion

# Predicción en la muestra de prueba
test_pred <- predict(model3, test_data, type = "response")
test_pred_class <- ifelse(test_pred > 0.5, "yes", "no")

# Matriz de confusión en la muestra de prueba
test_actual <- test_data$y
test_confusion <- table(Actual = test_actual, Predicted = test_pred_class)
test_confusion
```

```{r}
# Load required packages
library(ResourceSelection)  # For Hosmer-Lemeshow test
library(car)  # For VIF calculation
library(pROC)  # For ROC analysis

# 1. Residual analysis
par(mfrow = c(1, 2))  # Set up a 1x2 plot layout
plot(model4, which = 1, main = "Pearson Residuals vs Predicted Values")
plot(model4, which = 2, main = "Deviance Residuals vs Predicted Values")
```

### Goodness of fit

Goodness-of-fit refers to the assessment of how well a statistical model fits the observed data. When it comes to binomial models, the goodness-of-fit analysis aims to evaluate the adequacy of the model in capturing the relationship between the predictors and the binary response variable.

One commonly used metric for assessing the goodness-of-fit of a binomial model is the Hosmer-Lemeshow test. This test compares the observed outcomes with the predicted probabilities from the model. It divides the data into groups based on the predicted probabilities and assesses whether the observed frequencies within each group differ significantly from the expected frequencies.

The Hosmer-Lemeshow test provides a chi-square statistic and a corresponding p-value. A low p-value suggests a lack of fit between the model and the observed data, indicating that the predicted probabilities do not align well with the actual binary outcomes.

However, it's important to note that the Hosmer-Lemeshow test has some limitations and intricacies. First, it assumes that the predicted probabilities accurately represent the underlying probabilities in the population. If the model suffers from misspecification or violates certain assumptions, such as linearity or independence, the Hosmer-Lemeshow test may produce unreliable results.

```{r}
# 2. Goodness-of-fit test (Hosmer-Lemeshow)

# Calculate the observed and expected values
observed <- ifelse(train_data$y == "yes", 1,0)
expected <- fitted(model3)
expected <- ifelse(expected > 0.5, 1, 0)

# Perform the Hosmer-Lemeshow test
hoslem_result <- hoslem.test(observed, expected)
hoslem_result
```

The Hosmer-Lemeshow goodness-of-fit (GOF) test is commonly used in logistic regression to assess how well a model fits the observed data. It evaluates the agreement between the observed outcomes and the predicted probabilities from the model.

The test works by dividing the data into several groups based on the predicted probabilities. Then, it compares the observed and expected frequencies of the binary outcomes within each group. If the model fits the data well, the observed and expected frequencies should be similar across all groups.

1.  X-squared (Chi-square) Statistic:
    -   The X-squared statistic is a measure of the discrepancy between the observed and expected values in the contingency table.
    -   In this case, the X-squared value is 2.6101. A higher X-squared value suggests a larger discrepancy between the observed and expected values.
2.  Degrees of Freedom (df):
    -   The degrees of freedom for the Hosmer-Lemeshow test is determined by the number of categories in the contingency table minus 1.
    -   In this output, the degrees of freedom is 8, indicating that there are 8 categories being compared.
3.  p-value:
    -   The p-value assesses the statistical significance of the difference between the observed and expected values.
    -   In this case, the p-value is 0.9564, which is relatively high.
    -   A high p-value indicates that there is no significant discrepancy between the observed and expected values, suggesting that the logistic regression model (model3) fits the data well.

Implications: Based on the output of the Hosmer-Lemeshow GOF test for model3: - The X-squared statistic of 2.6101 indicates that there is a small discrepancy between the observed and expected values, but it is not statistically significant. - The high p-value of 0.9564 suggests that there is no evidence to reject the null hypothesis, which means that the logistic regression model fits the data well. - Therefore, we can conclude that model3 provides a good fit to the data based on the Hosmer-Lemeshow GOF test results.

```{r}
# 3. Influential observations (Cook's distance and leverage)
cooks_dist <- cooks.distance(model4)
hat_values <- hatvalues(model4)
plot(cooks_dist, pch = 19, main = "Cook's Distance")
plot(hat_values, pch = 19, main = "Leverage")

# Calculate Cook's distance values
cooks_dist <- cooks.distance(model2)

# Identify the indices of the 20 largest Cook's distances
top_influential <- order(cooks_dist, decreasing = TRUE)[1:20]

# Print the indices of the 20 most influential observations
print(top_influential)
```

```{r}
# 4. Collinearity assessment (Variance Inflation Factor)
vif_values <- vif(model4)
print(vif_values)
```

```{r}
# 6. ROC curve and AUC
model2_roc_data <- roc(train_data$y, predict(model2, type = "response"))
model3_roc_data <- roc(train_data$y, predict(model3, type = "response"))
```

```{r}
cat("Model2: ")
auc(model2_roc_data)
cat("Model3: ")
auc(model3_roc_data)
```

```{r}
par(mfrow=c(1,2))
plot(model2_roc_data, main = "Model 2 ROC Curve")
mtext(sprintf("Area under the curve: %0.5f", auc(model2_roc_data)), side = 4, line = 0)
plot(model3_roc_data, main = "Model 3 ROC Curve")
mtext(sprintf("Area under the curve: %0.5f", auc(model3_roc_data)), side = 4, line = 0)

```

The statement refers to the Area Under the Curve (AUC) value obtained from analyzing a Receiver Operating Characteristic (ROC) curve. The AUC measures the overall performance of a binary classification model, summarizing its ability to distinguish between the positive and negative classes.

In this case, the AUC value is 0.9965. An AUC of 1 represents a perfect classifier, while an AUC of 0.5 indicates a random classifier. Therefore, an AUC of 0.9965 is quite high, indicating that the model has a strong discriminatory power and performs exceptionally well in distinguishing between the positive and negative classes.

Based on the provided AUC value, it can be concluded that the model has excellent predictive performance. It demonstrates a high true positive rate and a low false positive rate, suggesting that it is capable of accurately classifying instances in the dataset. The closer the AUC is to 1, the better the model's performance is in terms of its ability to discriminate between the classes.
