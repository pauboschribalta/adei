---
title: "Entrega-2"
author: "Ivan Cala Mesa, Pau Bosch Ribalta"
date: "2023-03-28"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))

requiredPackages <- c("chemometrics","FactoMineR","car","knitr","missMDA")

package.check <- lapply(requiredPackages, FUN = function(x) {
  for (x in requiredPackages) {
    if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
  }
})
```

## R Markdown

Obtenim les dades:

```{r dataset, include=TRUE}
df <- read.csv("./bank-additional-clean.csv")
```

## AnÃ lisis PCA

```{r}
# Load the required packages
library(dplyr)
library(factoextra)
library(FactoMineR)

# Select numeric variables
df_numeric <- select(df, which(sapply(df, is.numeric)))

# Remove na_count variable
df_numeric <- df_numeric[, -which(names(df_numeric) == "na_count")]

# Create data frame for supplementary variables
df_numeric$y <- ifelse(df$y == "yes", 1, 0)

# Perform PCA with y as a supplementary variable
pca_result <- PCA(df_numeric, quanti.sup = c(8), graph = FALSE)


```

Multivariant outliers should be included as supplementary observations ll \<- which( df\$mout == "YesMOut") res.pca\<-PCA(df[,c(vars_res, vars_dis, vars_con)],quali.sup=c(1:19),quanti.sup= c(21), ind.sup = ll ) W plot.PCA(res.pca,choix=c("var"),invisible=c("quanti.sup")) plot.PCA(res.pca,choix=c("var"),invisible=c("var")) plot.PCA(res.pca,choix=c("var"),invisible=c("quanti.sup","var")) plot.PCA(res.pca,choix=c("ind"),invisible=c("ind"))

### 1. Eigenvalues and dominant axes. How many axes we have to interpret?

Kaiser's rule suggests that we should interpret all the axes with an eigenvalue greater than 1, while the elbow rule suggests that we should interpret the first few axes up to the point where the eigenvalues start to level off.

```{r}
library(FactoMineR)
library(factoextra)
# extract the eigenvalues
# Extract the eigenvalues and dominant axes
eigenvalues <- pca_result$eig
eigenvalues
eigenvalues[1:8,1]
kaiser_num_axes = length(which(eigenvalues[1:8,1]>1))
# print the results
cat("Number of axes to interpret using Kaiser's rule:", kaiser_num_axes, "\n")

```

According to Kaiser's rule, only 2 axes should be considered for the analysis.

```{r}
#dev.off()  # Close any previous plot windows
#fviz_pca_biplot(pca_result, col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
                #show.points = TRUE, show.labels = FALSE, label.size = 0)

```

```{r}
# creates a scree plot that shows the proportion of variance explained by each axis.
fviz_eig(pca_result, addlabels = TRUE) +
  ggtitle("PCA: Scree Plot of Eigenvalues") +
  scale_shape_manual(values = c(16, 17, 15)) +
  scale_color_manual(values = c("#FFA500", "#008000", "#0000FF"))

```

The scree plot created by fviz_eig() function shows the proportion of variance explained by each axis. The x-axis represents the axis number, and the y-axis represents the proportion of variance explained by that axis. In general, we want to retain as many axes as necessary to explain a reasonable proportion of the variance in the data. We can use Kaiser's rule or the elbow rule to determine the number of axes to retain.

It can be observed a significant change in slope from dimension 2 onward, which, according to the elbow rule, means that for this analysis we should consider only the first 2 dimensions.

Both rules suggest that only the first 2 axes are to be considered.

```{r}
# creates a correlation circle plot that shows the correlation between each variable and each axis.
fviz_pca_var(pca_result, col.var = "cos2", col.ind = "black")
```

The correlation circle plot created by fviz_pca_var() function shows the correlation between each variable and each axis. The x-axis and y-axis represent the first two axes, and the location of each variable on the plot represents its correlation with those axes. The length of the vector for each variable represents the correlation between that variable and the origin (i.e., the center of the plot). The angle between the vectors represents the correlation between the two variables. We can use this plot to identify which variables are most strongly associated with each axis and which variables are strongly correlated with each other.

### 2. Individuals point of view

```{r}
library(ggplot2)
# Extract the individual coordinates and squared distances from the PCA result
ind_coord <- get_pca_ind(pca_result)$coord
ind_dist <- get_pca_ind(pca_result)$cos2

# Plot the individual coordinates on the first two axes
fviz_pca_ind(pca_result, axes = 1:2, show.labels = FALSE, labelsize = 0) +
  ggtitle("PCA: Individual Coordinates on PC1 and PC2") +
  theme(text = element_text(size = 14)) +
  scale_shape_manual(values = c(16, 17, 15)) +
  scale_color_gradient2(low="darkslateblue", mid="white",
                      high="red", midpoint=0.5)
  
```

#### In dimension 1:

```{r}
rang<-order(pca_result$ind$coord[,1])
contrib.extremes<-c(row.names(df)[rang[1]], row.names(df)[rang[length(rang)]])
contrib.extremes<-c(row.names(df)[rang[1:10]], row.names(df)[rang[(length(rang)-10):length(rang)]])
fviz_pca_ind(pca_result, select.ind = list(names=contrib.extremes))
```

We can now have a look at them:

```{r}
df[which(row.names(df) %in% row.names(df)[rang[length(rang)]]), 1:length(df)]
df[which(row.names(df) %in% row.names(df)[rang[1]]),1:length(df)]
```

#### In dimension 2:

```{r}
# Select the 100 most extreme individuals based on the first principal component
rang1 <- order(pca_result$ind$coord[, 1])
contrib.extremes1 <- c(row.names(df)[rang1[1:50]], row.names(df)[rang1[(length(rang1) - 49):length(rang1)]])

# Select the 10 most extreme individuals based on the second principal component
rang2 <- order(pca_result$ind$coord[, 2])
contrib.extremes2 <- row.names(df)[rang2[1:10]]

# Combine the two sets of extreme individuals
contrib.extremes <- unique(c(contrib.extremes1, contrib.extremes2))

# Plot the extreme individuals
fviz_pca_ind(pca_result, select.ind = list(names = contrib.extremes))
```

We can now have a look at them:

```{r}
df[which(row.names(df) %in% row.names(df)[rang[length(rang)]]), 1:length(df)]
df[which(row.names(df) %in% row.names(df)[rang[1]]),1:length(df)]
```

```{r}

# Create a factor variable for the color-coded target variable
habillage <- factor(df$y)

# Plot the individuals with color-coded target variable
ggplot(data = data.frame(ind_coord, habillage), aes(x = Dim.1, y = Dim.2, color = habillage)) +
  geom_point() +
  ggtitle("PCA: Individuals Point of View")
```

-   This plot shows the positions of the individuals in the first two principal components. Each individual is represented by a point, and the color of the point represents the value of the categorical target variable.
-   The x-axis represents the first principal component, and the y-axis represents the second principal component. These two components together explain the most variance in the data.
-   The plot can help identify patterns or clusters of individuals in the data. For example, if individuals with the same value of the target variable tend to cluster together, this could indicate a relationship between the target variable and the principal components.

```{r}
# Identify extreme individuals
extreme_ind <- ind_coord[which(row.names(ind_coord) %in% contrib.extremes), ]
# Plot the extreme individuals on the first two axes
ggplot(data = data.frame(ind_coord), aes(x = Dim.1, y = Dim.2)) +
  geom_point() +
  geom_point(data = data.frame(extreme_ind), color = "red", size = 3)
  ggtitle("Extreme Individuals")
```

-   This plot shows the positions of the extreme individuals on the first two principal components. Each extreme individual is represented by a red point, and the non-extreme individuals are represented by black points.
-   The plot can help identify individuals that are outliers in the data or have extreme values on one or more of the principal components. These individuals may have a large influence on the PCA results and should be examined further to understand their characteristics and potential impact on the analysis.

```{r}
# Detect multivariate outliers
outliers <- which(rowSums(ind_dist) > mean(rowSums(ind_dist)) + 3 * sd(rowSums(ind_dist)))

# Print the extreme individuals and multivariate outliers
cat("Extreme individuals:\n")
print(extreme_ind)
cat("\n")

cat("Multivariate outliers:\n")
print(outliers)
```

### 3. Interpreting the axes: Variables point of view

The variance plot and contribution plot created by fviz_contrib() function show the contribution of each variable to each axis. In the variance plot, the x-axis represents the axis number, and the y-axis represents the contribution of each variable to that axis. In the contribution plot, the x-axis represents the variable name, and the y-axis represents the contribution of each variable to each axis. The contribution of each variable to each axis is measured by the squared correlation between the variable and the axis. We can use these plots to identify the variables that are most strongly associated with each axis.

```{r}
# Dimension description
dimdesc_result <- dimdesc(pca_result)

# Print the dimension description results
print(dimdesc_result$Dim.1)
# creates a variance plot that shows the contribution of each variable to each axis, and a contribution plot that shows the most contributing variables to each axis.
fviz_contrib(pca_result, choice = "var", axes = 1) +
  ggtitle("PCA: Contributions of Variables to PC1") +
  theme(text = element_text(size = 14)) +
  scale_shape_manual(values = c(16, 17, 15)) +
  scale_color_manual(values = c("#FFA500", "#008000", "#0000FF"))
```

The output shows the correlation coefficients and p-values between the variables and the continuous variables in the first dimension of the PCA analysis.

The most important correlations in the first dimension are:

-   The negative correlation between the target variable "y" and the first dimension (-0.695), which means that this variable is strongly associated with the opposite direction of the first principal component.

-   The negative correlation between "duration" and the first dimension (-0.256), indicating that longer call durations are associated with the opposite direction of the first principal component.

-   The positive correlations between "euribor3m" (0.976), "emp.var.rate" (0.973), "cons.price.idx" (0.932), "cons.conf.idx" (0.925), and "nr.employed" (0.884) with the first dimension, which suggests that these economic indicators are associated with the same direction of the first principal component.

The small p-values (all are 0.000) indicate that these correlations are statistically significant. The age of the customer ("age_num") and the number of contacts performed during the current campaign ("campaign") also show some correlation with the first dimension, although to a lesser extent than the other variables.

```{r}
# Print the dimension description results
print(dimdesc_result$Dim.2)
# creates a variance plot that shows the contribution of each variable to each axis, and a contribution plot that shows the most contributing variables to each axis.
fviz_contrib(pca_result, choice = "var", axes = 2) +
  ggtitle("PCA: Contributions of Variables to PC2") +
  theme(text = element_text(size = 14)) +
  scale_shape_manual(values = c(16, 17, 15)) +
  scale_color_manual(values = c("#FFA500", "#008000", "#0000FF"))
```

This output shows the link between the variables and the continuous variables in terms of the R-square correlation and p-value. The most important correlations for the second dimension are:

-   Duration: this variable has the highest correlation with the second dimension, meaning that it contributes the most to the definition of this axis.

-   y: this variable also has a significant correlation with the second dimension, indicating that it is relevant for the analysis of this axis.

-   nr.employed and campaign: these variables have a moderate correlation with the second dimension, suggesting that they are also relevant to some extent.

-   emp.var.rate and euribor3m: these variables have a weaker correlation with the second dimension, but still contribute to its definition.

-   cons.price.idx, age_num, and cons.conf.idx: these variables have a negligible correlation with the second dimension, meaning that they do not play an important role in the analysis of this axis.

Overall, this output indicates that the second dimension is mostly defined by the duration variable, followed by other variables related to the campaign and employment, while other variables have a minor influence on this dimension.

So, in this case, we have run PCA on a dataset and extracted the first and second principal components, which explain 62% and 18.6% of the total variance, respectively.

The interpretation of the first dimension suggests a strong link between economic indicators, such as euribor3m, emp.var.rate, cons.price.idx, cons.conf.idx, and nr.employed, and the target variable y, as they show high positive correlation. Conversely, the variable duration shows negative correlation, which suggests that it has an inverse relationship with the economic indicators and y. In this sense, the first dimension can be interpreted as an economic indicator.

On the other hand, the second dimension is mainly driven by the variable duration and target variable y, which show high positive correlation, while the economic indicators have a much lower correlation. This suggests that duration and y are important features for distinguishing the data points along the second dimension, which could be interpreted as a behavioral indicator.

The first dimension appears to capture the economic aspects of the data,
while the second dimension reflects the behavioral aspect. Although the
first dimension explains more variance, the second dimension is also
important as it captures different aspects of the data that are not
captured by the first dimension. Therefore, considering both dimensions
is crucial for a comprehensive understanding of the dataset.

```{r}


```

```{r}


```

```{r}
# Representation of clouds
fviz_pca_var(pca_result, axes = c(1,2 ), col.var = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, ggtheme = theme_classic())
```

### 4. Perform a PCA taking into account also supplementary variables

```{r}
#c(vars_res, vars_dis,vars_con)
#ll <- which( df$mout == "YesMOut")
#res.pca<-PCA(df[,c(vars_res, vars_con)],quali.sup=c(1),quanti.sup= c(3), ind.sup = ll) 
#plot(res.pca, choix="ind",invisible=c("ind","ind.sup"), cex=0.7, graph.type = "classic")
# 
#lines(res.pca$quali.sup$coord[1:2,1],res.pca$quali.sup$coord[1:2,2],lwd=2,col="black") # Does not work unless graph.type "classic" is set

# Manually producing the plot
#plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2],pch=19,col="grey30")
#points(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],pch=15,col="magenta")
# lines(res.pca$quali.sup$coord[7:10,1],res.pca$quali.sup$coord[7:10,2],lwd=2,lty=2,col="blue")
#text(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],labels=names(res.pca$quali.sup$coord[,1]),col="magenta",cex=0.8)

#res.pca$quali.sup$coord


```

## Clustering PCA

### K-Means classification

```{r}
prc<-pca_result$ind$coord[,1:3] # 3 components principals (kaiser)
dim(prc)
```
### Classification

```{r}
dist<-dist(prc)  # coordenates are real - Euclidean metric
kc<-kmeans(dist, 5, iter.max=30, trace=TRUE) #caclulate the distances, it turns into a matrix
```
We see from the output that in 4 iterations it has converged.
We now procceed to save in the data frame the number of clusters.
```{r}
df_numeric$claKM<-0
df_numeric$claKM<-kc$cluster
df_numeric$claKM<-factor(df_numeric$claKM)
barplot(table(df$claKM),col="darkslateblue",border="darkslateblue",main="[k-means]#observations/cluster")
```
```{r}
dim(df_numeric)
cat.res <-catdes(df_numeric,grep("^claKM$", colnames(df_numeric)))

```
The output shows the results of a cluster analysis based on the K-means algorithm. The analysis was carried out using eight quantitative variables and a categorical variable 'y' representing whether or not the client subscribed to a product.

The first table shows the Eta squared value and p-value for each variable in relation to the cluster variable. The Eta squared value indicates the proportion of variance explained by the cluster variable, and the p-value indicates the statistical significance of the relationship between the variables. All variables showed a significant relationship with the cluster variable except for age_num.

The second table provides a detailed description of each cluster based on the quantitative variables. The analysis resulted in five distinct clusters.

Cluster 1 is characterized by a high duration of calls, low number of campaigns, high consumer confidence, low consumer price index, low consumer confidence index, and low number of employees and employment variation rate. This group represents a small proportion of customers, but stands out for its high call duration, which may indicate a high-quality interaction between the customer and the company.

Cluster 2 is characterized by an extremely high call duration, high employment rate, high employment variation rate, high euribor value, positive employment variation rate, and high subscription rate to the product. This group is also an attractive target for advertising campaigns.

Cluster 3 is characterized by low call duration, low product subscription rate, high consumer confidence index, high consumer price index, and high number of employees. This group represents the majority of customers. The low call duration and low number of subscriptions may suggest that this group is less receptive to advertising campaigns, although their high consumer confidence index may be an opportunity for brand building and customer loyalty.

Cluster 4 is characterized by a high subscription rate to the product and low values of employment variation rate, consumer confidence index, consumer price index, and euribor. This group may represent loyal customers or customers with a high level of satisfaction with the product or service.

Cluster 5 is characterized by high subscription rates to the product, high employment rate, high call duration, high euribor value, high employment variation rate, and high consumer price index. This group may represent a high-value target for advertising campaigns due to their high subscription rate and favorable economic indicators.


### Hierarchical clustering

```{r}
# Perform hierarchical clustering on the first two principal components
#hclust_result <- hclust(dist(pca_result$ind$coord[, 1:2]))

# Visualize the results using a dendrogram
#fviz_dend(hclust_result, cex = 0.5, k = 2)

# Cut the dendrogram into three clusters
#cut_result <- cutree(hclust_result, k = w)

# Describe the clusters
#summary(cut_result)

res.hcpc <- HCPC(pca_result,nb.clust = -1, order = TRUE)
table(res.hcpc$data.clust$clust)
barplot(table(res.hcpc$data.clust$clust), col="darkslateblue", border="darkslateblue", main="[hierarchical] #observations/cluster")
```
```{r}
  # categorical variables which characterizes the clusters
```

```{r}
res.hcpc$desc.var$quanti.var    # description of each cluster by the categories
res.hcpc$desc.var$quanti
```

Per a relitzar la descripciÃ³ dels grups d'individus, hem de realitzar una agrupaciÃ³ jerÃ rquica dels components principals (HCPC).

Agafem `r res.hcpc$call$t$nb.clust` clusters, ja que sÃ³n els que ens indica el propi HCPC que hem d'incloure degut a la inÃ¨rcia acumulada d'aquests.

A la segÃ¼ent grÃ fica es pot veure les inÃ¨rcies per cada parella de clusters. Veiem que les mÃ©s significatives sÃ³n de la 1 a la 2 i en menor mesura la 3 (les que ens recomanava agafar el HCPC).

```{r}
#fviz_dend(res.hcpc, show_labels = FALSE)
plot(res.hcpc, choice = "bar")
```

A continuaciÃ³ imprimirem en un factor map tots els individus agrupats amb els diferents clusters que tenim. Podem veure com el cluster 1 esta completament difgerenciat de la resta, i el cluster 2 estÃ  disper i te punts desviats que provoquen que abarquin molta superfÃ­cie sense individus.

```{r}
fviz_cluster(res.hcpc, geom = "point")
```

A continuaciÃ³ durem a terme la descripciÃ³ de clusters envers les variables i categories mÃ©s rellevants en ells.

Primer de tot veiem les variables mÃ©s relacionades amb tots els clusters:

```{r}
res.hcpc$desc.var
```
The analysis is based on a cluster analysis and its description in terms of quantitative variables. In this case, eight variables have been used for the analysis. The table describes the link between the cluster variable and the quantitative variables, followed by the description of each cluster individually.

The variable "y" (customer subscription to the product) has a very strong relationship with the cluster variables in all three groups, with an Eta2 relationship of 98%, 98%, and 98% respectively. This indicates that customer subscription is an important factor in the separation of clusters.

Cluster 1 is described by a high level of call duration, a low number of campaigns, high consumer confidence, a low consumer price index, low consumer confidence index, and a low number of employees and employment rate variation. This group represents a small proportion of customers but stands out for its high call duration, which may indicate high-quality interaction between the customer and the company.

Cluster 2 is characterized by an extremely high call duration, a high employment rate, high employment rate variation, high euribor value, and positive employment rate variation. This group also has a high product subscription rate, indicating that this group is an attractive target for advertising campaigns.

Cluster 3 is described by low call duration, low product subscription rate, high consumer confidence index, high consumer price index, and high number of employees. This group represents the majority of customers. The low call duration and low number of subscriptions may suggest that this group is less receptive to advertising campaigns, although their high consumer confidence may be an opportunity for brand building and customer loyalty.

## AnÃ lisis CA

```{r}
# Convert the 'duration' variable to a factor with 7 levels
df$duration_cat <- cut(df$duration, breaks = 7, labels = FALSE)



# Perform Correspondence Analysis on the data
#ca_result <- CA(df_numeric, graph = FALSE, ncp = 2)

# Visualize the results using a biplot
#fviz_ca_biplot(ca_result, repel = TRUE)
```

```{r}
# Describe the results
#summary(ca_result)
```

## AnÃ lisis MCA

## Clustering MCA

##ComparaciÃ³ de clustering PCA-MCA
