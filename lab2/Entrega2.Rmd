---
title: "Entrega-2"
author: "Ivan Cala Mesa, Pau Bosch Ribalta"
date: '2023-03-28'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))

requiredPackages <- c("chemometrics","FactoMineR","car","knitr","missMDA")

package.check <- lapply(requiredPackages, FUN = function(x) {
  for (x in requiredPackages) {
    if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
  }
})
```

## R Markdown

Obtenim les dades:

```{r dataset, include=TRUE}
df <- read.csv("./bank-additional-clean.csv")
```

## Anàlisis PCA

```{r}
# Load the required packages
library(dplyr)

# Select only the numeric variables from the modified dataset
df_numeric <- df %>% select_if(is.numeric)
df_scaled <- scale(df_numeric)
pca_result <- PCA(df_scaled, graph = FALSE)
```

Multivariant outliers should be included as supplementary observations ll \<- which( df\$mout == "YesMOut") res.pca\<-PCA(df[,c(vars_res, vars_dis, vars_con)],quali.sup=c(1:19),quanti.sup= c(21), ind.sup = ll ) W plot.PCA(res.pca,choix=c("var"),invisible=c("quanti.sup")) plot.PCA(res.pca,choix=c("var"),invisible=c("var")) plot.PCA(res.pca,choix=c("var"),invisible=c("quanti.sup","var")) plot.PCA(res.pca,choix=c("ind"),invisible=c("ind"))

### 1. Eigenvalues and dominant axes. How many axes we have to interpret?

Kaiser's rule suggests that we should interpret all the axes with an eigenvalue greater than 1, while the elbow rule suggests that we should interpret the first few axes up to the point where the eigenvalues start to level off.

```{r}
library(FactoMineR)
library(factoextra)
# extract the eigenvalues
# Extract the eigenvalues and dominant axes
eigenvalues <- pca_result$eig
total_var <- sum(eigenvalues)
prop_var <- eigenvalues / total_var
cum_var <- cumsum(prop_var)
elbow_num_axes <- sum(cum_var <= 0.9)
kaiser_num_axes <- sum(eigenvalues > 1)

# print the results
cat("Number of axes to interpret using Kaiser's rule:", kaiser_num_axes, "\n")
cat("Number of axes to interpret using the elbow rule:", elbow_num_axes, "\n")
```

```{r}
# creates a scree plot that shows the proportion of variance explained by each axis.
fviz_eig(pca_result, addlabels = TRUE)
```

The scree plot created by fviz_eig() function shows the proportion of variance explained by each axis. The x-axis represents the axis number, and the y-axis represents the proportion of variance explained by that axis. In general, we want to retain as many axes as necessary to explain a reasonable proportion of the variance in the data. We can use Kaiser's rule or the elbow rule to determine the number of axes to retain.

```{r}
# creates a variance plot that shows the contribution of each variable to each axis, and a contribution plot that shows the most contributing variables to each axis.
fviz_contrib(pca_result, choice = "var", axes = 1:2)
```

The variance plot and contribution plot created by fviz_contrib() function show the contribution of each variable to each axis. In the variance plot, the x-axis represents the axis number, and the y-axis represents the contribution of each variable to that axis. In the contribution plot, the x-axis represents the variable name, and the y-axis represents the contribution of each variable to each axis. The contribution of each variable to each axis is measured by the squared correlation between the variable and the axis. We can use these plots to identify the variables that are most strongly associated with each axis.

```{r}
# creates a correlation circle plot that shows the correlation between each variable and each axis.
fviz_pca_var(pca_result, col.var = "cos2", col.ind = "black")
```

The correlation circle plot created by fviz_pca_var() function shows the correlation between each variable and each axis. The x-axis and y-axis represent the first two axes, and the location of each variable on the plot represents its correlation with those axes. The length of the vector for each variable represents the correlation between that variable and the origin (i.e., the center of the plot). The angle between the vectors represents the correlation between the two variables. We can use this plot to identify which variables are most strongly associated with each axis and which variables are strongly correlated with each other.

### 2. Individuals point of view

```{r}
library(ggplot2)
# Extract the individual coordinates and squared distances from the PCA result
ind_coord <- get_pca_ind(pca_result)$coord
ind_dist <- get_pca_ind(pca_result)$cos2

# Plot the individual coordinates on the first two axes
fviz_pca_ind(pca_result, axes = 1:2)

# Identify extreme individuals
extreme_ind <- ind_coord[which(abs(ind_coord[,1]) > 3 | abs(ind_coord[,2]) > 3),]
```

```{r}
# Create a factor variable for the color-coded target variable
habillage <- factor(df$y)

# Plot the individuals with color-coded target variable
ggplot(data = data.frame(ind_coord, habillage), aes(x = Dim.1, y = Dim.2, color = habillage)) +
  geom_point() +
  ggtitle("PCA: Individuals Point of View")
```
- This plot shows the positions of the individuals in the first two principal components. Each individual is represented by a point, and the color of the point represents the value of the categorical target variable.
- The x-axis represents the first principal component, and the y-axis represents the second principal component. These two components together explain the most variance in the data.
- The plot can help identify patterns or clusters of individuals in the data. For example, if individuals with the same value of the target variable tend to cluster together, this could indicate a relationship between the target variable and the principal components.

```{r}
# Plot the extreme individuals on the first two axes
ggplot(data = data.frame(ind_coord), aes(x = Dim.1, y = Dim.2)) +
  geom_point() +
  geom_point(data = data.frame(extreme_ind), color = "red", size = 3) +
  ggtitle("Extreme Individuals")
```
- This plot shows the positions of the extreme individuals (those with coordinates greater than 3 on either axis) on the first two principal components. Each extreme individual is represented by a red point, and the non-extreme individuals are represented by black points.
- The plot can help identify individuals that are outliers in the data or have extreme values on one or more of the principal components. These individuals may have a large influence on the PCA results and should be examined further to understand their characteristics and potential impact on the analysis.

```{r}
# Detect multivariate outliers
outliers <- which(rowSums(ind_dist) > mean(rowSums(ind_dist)) + 3 * sd(rowSums(ind_dist)))

# Print the extreme individuals and multivariate outliers
cat("Extreme individuals:\n")
print(extreme_ind)
cat("\n")

cat("Multivariate outliers:\n")
print(outliers)
```

### 3. Interpreting the axes: Variables point of view

```{r}
# Dimension description
dimdesc_result <- dimdesc(pca_result)

# Print the dimension description results
print(dimdesc_result)

# Representation of clouds
fviz_pca_var(pca_result, col.var = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, ggtheme = theme_classic())
```

### 4. Perform a PCA taking into account also supplementary variables

```{r}
c(vars_res, vars_dis,vars_con)
ll <- which( df$mout == "YesMOut")
res.pca<-PCA(df[,c(vars_res, vars_con)],quali.sup=c(1),quanti.sup= c(3), ind.sup = ll) 
plot(res.pca, choix="ind",invisible=c("ind","ind.sup"), cex=0.7, graph.type = "classic")
# 
lines(res.pca$quali.sup$coord[1:2,1],res.pca$quali.sup$coord[1:2,2],lwd=2,col="black") # Does not work unless graph.type "classic" is set

# Manually producing the plot
plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2],pch=19,col="grey30")
points(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],pch=15,col="magenta")
# lines(res.pca$quali.sup$coord[7:10,1],res.pca$quali.sup$coord[7:10,2],lwd=2,lty=2,col="blue")
text(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],labels=names(res.pca$quali.sup$coord[,1]),col="magenta",cex=0.8)

res.pca$quali.sup$coord


```

## Clustering PCA

### K-Means classification

### Hierarchical clustering

## Anàlisis CA

## Anàlisis MCA

## Clustering MCA

##Comparació de clustering PCA-MCA
