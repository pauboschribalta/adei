---
title: "Entrega-2"
<<<<<<< HEAD
author: "Ivan Cala Mesa, Pau Bosch Ribalta"
date: "2023-03-28"
output:
  html_document:
    df_print: paged
=======
author: "Ivan Cala Mesa - Pau Bosch Ribalta"
date: "\today"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '4'
    df_print: paged
geometry: left=1.9cm,right=1.9cm,top=1.25cm,bottom=1.52cm
fontsize: 18pt
classoption: a4paper
editor_options:
  chunk_output_type: console
>>>>>>> origin/pau
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load Required Packages: to be increased over the course
options(contrasts=c("contr.treatment","contr.treatment"))

requiredPackages <- c("chemometrics","FactoMineR","car","knitr","missMDA",
                      "ggplot2", "factoextra")

package.check <- lapply(requiredPackages, FUN = function(x) {
  for (x in requiredPackages) {
    if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
  }
})
```

## R Markdown

Obtenim les dades i les classifiquem:

```{r dataset, include=TRUE}
setwd("/home/pau/Escriptori/adei/lab2")
load("./bank-additional-clean.RData")
```

```{r}
<<<<<<< HEAD
# Load the required packages
library(dplyr)
library(factoextra)
library(FactoMineR)

# Select numeric variables
df_numeric <- select(df, which(sapply(df, is.numeric)))

# Remove na_count variable
df_numeric <- df_numeric[, -which(names(df_numeric) == "na_count")]

# Create data frame for supplementary variables
df_numeric$y <- ifelse(df$y == "yes", 1, 0)

# Perform PCA with y as a supplementary variable
pca_result <- PCA(df_numeric, quanti.sup = c(8), graph = FALSE)


```

Multivariant outliers should be included as supplementary observations ll \<- which( df\$mout == "YesMOut") res.pca\<-PCA(df[,c(vars_res, vars_dis, vars_con)],quali.sup=c(1:19),quanti.sup= c(21), ind.sup = ll ) W plot.PCA(res.pca,choix=c("var"),invisible=c("quanti.sup")) plot.PCA(res.pca,choix=c("var"),invisible=c("var")) plot.PCA(res.pca,choix=c("var"),invisible=c("quanti.sup","var")) plot.PCA(res.pca,choix=c("ind"),invisible=c("ind"))

### 1. Eigenvalues and dominant axes. How many axes we have to interpret?
=======
var_dis <- c("age", "job", "marital", "education", "housing", "loan",
             "contact", "month", "day_of_week", "previous", "poutcome", 
             "mout")
var_con<- c("age_num", "duration", "campaign", "emp.var.rate", "cons.conf.idx", 
            "cons.price.idx", "euribor3m", "nr.employed", "na_count")
var_res<- c("y")
df$default <- NULL
```

## Anàlisis CA

### Transformació de la variable duration
>>>>>>> origin/pau

Kaiser's rule suggests that we should interpret all the axes with an eigenvalue greater than 1, while the elbow rule suggests that we should interpret the first few axes up to the point where the eigenvalues start to level off.

```{r}
<<<<<<< HEAD
library(FactoMineR)
library(factoextra)
# extract the eigenvalues
# Extract the eigenvalues and dominant axes
eigenvalues <- pca_result$eig
eigenvalues
eigenvalues[1:8,1]
kaiser_num_axes = length(which(eigenvalues[1:8,1]>1))
# print the results
cat("Number of axes to interpret using Kaiser's rule:", kaiser_num_axes, "\n")

```

According to Kaiser's rule, only 2 axes should be considered for the analysis.

```{r}
#dev.off()  # Close any previous plot windows
#fviz_pca_biplot(pca_result, col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
                #show.points = TRUE, show.labels = FALSE, label.size = 0)

```

```{r}
# creates a scree plot that shows the proportion of variance explained by each axis.
fviz_eig(pca_result, addlabels = TRUE) +
  ggtitle("PCA: Scree Plot of Eigenvalues") +
  scale_shape_manual(values = c(16, 17, 15)) +
  scale_color_manual(values = c("#FFA500", "#008000", "#0000FF"))

```

The scree plot created by fviz_eig() function shows the proportion of variance explained by each axis. The x-axis represents the axis number, and the y-axis represents the proportion of variance explained by that axis. In general, we want to retain as many axes as necessary to explain a reasonable proportion of the variance in the data. We can use Kaiser's rule or the elbow rule to determine the number of axes to retain.

It can be observed a significant change in slope from dimension 2 onward, which, according to the elbow rule, means that for this analysis we should consider only the first 2 dimensions.

Both rules suggest that only the first 2 axes are to be considered.

```{r}
# creates a correlation circle plot that shows the correlation between each variable and each axis.
fviz_pca_var(pca_result, col.var = "cos2", col.ind = "black")
```

The correlation circle plot created by fviz_pca_var() function shows the correlation between each variable and each axis. The x-axis and y-axis represent the first two axes, and the location of each variable on the plot represents its correlation with those axes. The length of the vector for each variable represents the correlation between that variable and the origin (i.e., the center of the plot). The angle between the vectors represents the correlation between the two variables. We can use this plot to identify which variables are most strongly associated with each axis and which variables are strongly correlated with each other.
=======
df$duration_fact <- cut(df$duration, 
              breaks = c(0, 10, 30, 60, 300, 900, 1800, max(df$duration)),
              labels = c("extr.curt", "molt.curta", "curta",
                         "normal", "llarga", "molt.llarga", "extr.llarga"))
df$duration_fact <- as.factor(df$duration_fact)

summary(df$duration_fact)
```

### Eigenvalues and dominant axes analysis

Realitzarem l'anàlisis per la target (duration_fact) i per les variables categòriques job i education

#### Duration_fact - job


Realitzem la taula que relaciona les dues variables i fem l'anàlisis de correspondència (CA).

```{r}
tab1 <- table(df[,c("duration_fact", "job")])
tab1
res.ca1 <- CA(tab1, graph = F)
```

Seguidament triarem les dimensions que hem d'agafar, gràficament i a partir dels eigenvalues.

```{r}
fviz_eig(res.ca1)

mm <- mean(res.ca1$eig[,1])
ll<- which(as.data.frame(res.ca1$eig[,1])>mm)
length(ll) #Número dimensions
res.ca1$eig[length(ll),3]
```

Gràficament, per la regla del colze, veiem que la dimensió on hi ha un canvi important de la corva és la 2. A més, per Kaiser, agafem totes les dimensions amb els eigenvalues els quals superin la mitjana de tots els eigenvalues, i també ens surten dues dimensions.

Amb dues dimensions representem un `r res.ca1$eig[length(ll),3]`%, un percentatge prou considerable.

```{r}
plot( res.ca1, cex=0.8, graph.type = "classic" )
lines( res.ca1$row$coord[,1], res.ca1$row$coord[,2], col="blue", lwd = 2 )
lines( res.ca1$col$coord[,1], res.ca1$col$coord[,2], col="red", lwd = 2 )
```

Tal i com podem veure a la gràfica, hi ha diverses categories amb valors molt similars, que, per tant, podriem considerar-les com a una sola. Per exemple la duration_fact curta i molt.curta tenen quasibé el mateix valor, la resta de categories tenen prou discrepància. Mencionar que en la variable job, tenim dues altres categories amb valors molt similars, que són services i self-employed.

Podem observar que les feines amb posicions superiors tendeixen a estar més estona a la trucada, mentres que els unemployed estan totalment separats.

#### Duration_fact - Education


Igual que amb la parella anterior, realitzem la taula que relaciona les dues variables i fem l'anàlisis de correspondència (CA).

```{r}
df$education <- factor(df$education, levels = c( "illiterate", "basic",
                                                 "high.school", 
                                                 "professional.course", 
                                                 "university.degree"))
tab2 <- table(df[,c("duration_fact", "education")])
tab2
res.ca2 <- CA(tab2, graph = F)
```

Seguidament triarem les dimensions que hem d'agafar, gràficament i a partir dels eigenvalues.

```{r}
fviz_eig(res.ca2)

mm <- mean(res.ca2$eig[,1])
ll<- which(as.data.frame(res.ca2$eig[,1])>mm)
length(ll) #Número dimensions
res.ca2$eig[length(ll),3]
```

Gràficament, per la regla del colze, veiem que la dimensió on hi ha un canvi important de la corva és la 1. A més, per Kaiser, agafem totes les dimensions amb els eigenvalues els quals superin la mitjana de tots els eigenvalues, i també ens surt una sola dimensió.

Amb aquesta dimensions representem un `r res.ca2$eig[length(ll),3]`%, de nou un percentatge prou considerable.

```{r}
plot( res.ca2, cex=0.8, graph.type = "classic" )
lines( res.ca2$row$coord[,1], res.ca2$row$coord[,2], col="blue", lwd = 2 )
lines( res.ca2$col$coord[,1], res.ca2$col$coord[,2], col="red", lwd = 2 )
```

De la mateixa forma que hem vist amb la parella de variables anterior, aquí tornem a veure que les categories de duration_fact curta i molt.curta tenen valors molt similars. A més, veiem que llarga i molt.llarga també els passa el mateix.

Ens podem fixar també amb que la categoria de education illiterate està molt separada de la resta, cosa que té molt de sentit. De la mateixa manera podem veure que els nivells d'estudi més alts estan relacionats amb les trucades més llargues.

## Anàlisis MCA

```{r}
llmout<-which(df$mout=="Yes")

res.mca<-MCA(df[,c(var_res, var_con[2], var_dis[1:11]) ],
             quali.sup= 1, quanti.sup = 2, ind.sup=llmout,
             graph = F)
```

### 1. Eigenvalues and dominant axes. How many axes we have to consider for next Hierarchical Classification stage?

En aquest primer punt haurem d'escollir les dimensions que agafem per fer l'anàlisis a partir dels eigenvalues. Per a triar les dimensions durem a terme dos mètodes, el de Kaiser i el de la regla del colze:

#### Regla de Kaiser

La regla de Kaiser ens diu que haurem d'agafar totes aquelles dimensions amb el valor del eigenvalue superior al de la mitjana d'eigenvalues de totes les dimensions.

```{r}
summary(res.mca, nbelements = 12, nbind = 0)
mm <- mean(res.mca$eig[,1])
ll<- which(as.data.frame(res.mca$eig[,1])>mm)
length(ll) #Número dimensions
res.mca$eig[length(ll),3]

barplot(res.mca$eig[,1],
        main="valors propis",
        names.arg=paste("dim",1:nrow(res.mca$eig)),
        las = 2,
        ylim = c(0, 0.25),
        col = "blue4")

abline(h = mm,
       col = "red",
       lty = "dashed")
```

Per la regla de Kaiser ens surten `r length(ll)` dimensions, però el percentatge explicat és `r res.mca$eig[length(ll),3]`%, un percentatge que considerem baix.

#### Regla del colze

La regla del colze ens diu que hem d'agafar la dimensió la qual fa variar la corva de la gràfica que ens indica el valor propi de cada dimensió:

```{r}
res.mca$eig
fviz_screeplot(res.mca,
               ylim = c(0, 8),
               ncp = 33)
```

En el nostre cas, agafarem la primera dimensió que té un percentatge acomulat de variança més gran de 85%, la dimensió 24. Podem veure gràficament com aquesta dimensió és la última que manté una corva de valor propi constant i que ens explica suficient variança, a partir de la dimensió 25 la corva canvia la seva linealitat.
>>>>>>> origin/pau

### 2. Individuals point of view

```{r}
<<<<<<< HEAD
library(ggplot2)
# Extract the individual coordinates and squared distances from the PCA result
ind_coord <- get_pca_ind(pca_result)$coord
ind_dist <- get_pca_ind(pca_result)$cos2

# Plot the individual coordinates on the first two axes
fviz_pca_ind(pca_result, axes = 1:2, show.labels = FALSE, labelsize = 0) +
  ggtitle("PCA: Individual Coordinates on PC1 and PC2") +
  theme(text = element_text(size = 14)) +
  scale_shape_manual(values = c(16, 17, 15)) +
  scale_color_gradient2(low="darkslateblue", mid="white",
                      high="red", midpoint=0.5)
  
```

#### In dimension 1:

```{r}
rang<-order(pca_result$ind$coord[,1])
contrib.extremes<-c(row.names(df)[rang[1]], row.names(df)[rang[length(rang)]])
contrib.extremes<-c(row.names(df)[rang[1:10]], row.names(df)[rang[(length(rang)-10):length(rang)]])
fviz_pca_ind(pca_result, select.ind = list(names=contrib.extremes))
```

We can now have a look at them:

```{r}
df[which(row.names(df) %in% row.names(df)[rang[length(rang)]]), 1:length(df)]
df[which(row.names(df) %in% row.names(df)[rang[1]]),1:length(df)]
```

#### In dimension 2:

```{r}
# Select the 100 most extreme individuals based on the first principal component
rang1 <- order(pca_result$ind$coord[, 1])
contrib.extremes1 <- c(row.names(df)[rang1[1:50]], row.names(df)[rang1[(length(rang1) - 49):length(rang1)]])

# Select the 10 most extreme individuals based on the second principal component
rang2 <- order(pca_result$ind$coord[, 2])
contrib.extremes2 <- row.names(df)[rang2[1:10]]

# Combine the two sets of extreme individuals
contrib.extremes <- unique(c(contrib.extremes1, contrib.extremes2))

# Plot the extreme individuals
fviz_pca_ind(pca_result, select.ind = list(names = contrib.extremes))
```

We can now have a look at them:

```{r}
df[which(row.names(df) %in% row.names(df)[rang[length(rang)]]), 1:length(df)]
df[which(row.names(df) %in% row.names(df)[rang[1]]),1:length(df)]
```

```{r}

# Create a factor variable for the color-coded target variable
habillage <- factor(df$y)

# Plot the individuals with color-coded target variable
ggplot(data = data.frame(ind_coord, habillage), aes(x = Dim.1, y = Dim.2, color = habillage)) +
  geom_point() +
  ggtitle("PCA: Individuals Point of View")
```

-   This plot shows the positions of the individuals in the first two principal components. Each individual is represented by a point, and the color of the point represents the value of the categorical target variable.
-   The x-axis represents the first principal component, and the y-axis represents the second principal component. These two components together explain the most variance in the data.
-   The plot can help identify patterns or clusters of individuals in the data. For example, if individuals with the same value of the target variable tend to cluster together, this could indicate a relationship between the target variable and the principal components.

```{r}
# Identify extreme individuals
extreme_ind <- ind_coord[which(row.names(ind_coord) %in% contrib.extremes), ]
# Plot the extreme individuals on the first two axes
ggplot(data = data.frame(ind_coord), aes(x = Dim.1, y = Dim.2)) +
  geom_point() +
  geom_point(data = data.frame(extreme_ind), color = "red", size = 3)
  ggtitle("Extreme Individuals")
```

-   This plot shows the positions of the extreme individuals on the first two principal components. Each extreme individual is represented by a red point, and the non-extreme individuals are represented by black points.
-   The plot can help identify individuals that are outliers in the data or have extreme values on one or more of the principal components. These individuals may have a large influence on the PCA results and should be examined further to understand their characteristics and potential impact on the analysis.

```{r}
# Detect multivariate outliers
outliers <- which(rowSums(ind_dist) > mean(rowSums(ind_dist)) + 3 * sd(rowSums(ind_dist)))

# Print the extreme individuals and multivariate outliers
cat("Extreme individuals:\n")
print(extreme_ind)
cat("\n")

cat("Multivariate outliers:\n")
print(outliers)
=======
plot(res.mca, choix = c("ind"),
     invisible = c("var", "quali.sup"),
     cex = 1)
```

Podem distingir dos grups diferenciats d'individus, un a l'origen de coordenades i l'altre al primer quadrant, i un grup molt petit d'individus entre ells. Tal i com veiem a la gràfica, el grup del primer quadrant té una contribució molt superior als altres tan en la dimensió 1 com en la 2.

A continuació veurem els 10 individus que més contribueixen a explicar la primera dimensió i quins valors tenen en les diferents variables:

```{r}
inds <- res.mca$ind$coord
inds <- as.data.frame(inds)
rang<-inds[order(inds$`Dim 1`, decreasing = TRUE),]
res.mca$ind$coord[row.names(rang)[1:10],1]
df[which(row.names(df) %in% row.names(res.mca$ind$coord
                                      [row.names(rang)[1:10],])),1:20]
>>>>>>> origin/pau
```

Seguidament veurem la mateixa informació però per la segona dimensió:

The variance plot and contribution plot created by fviz_contrib() function show the contribution of each variable to each axis. In the variance plot, the x-axis represents the axis number, and the y-axis represents the contribution of each variable to that axis. In the contribution plot, the x-axis represents the variable name, and the y-axis represents the contribution of each variable to each axis. The contribution of each variable to each axis is measured by the squared correlation between the variable and the axis. We can use these plots to identify the variables that are most strongly associated with each axis.

```{r}
<<<<<<< HEAD
# Dimension description
dimdesc_result <- dimdesc(pca_result)

# Print the dimension description results
print(dimdesc_result$Dim.1)
# creates a variance plot that shows the contribution of each variable to each axis, and a contribution plot that shows the most contributing variables to each axis.
fviz_contrib(pca_result, choice = "var", axes = 1) +
  ggtitle("PCA: Contributions of Variables to PC1") +
  theme(text = element_text(size = 14)) +
  scale_shape_manual(values = c(16, 17, 15)) +
  scale_color_manual(values = c("#FFA500", "#008000", "#0000FF"))
```

The output shows the correlation coefficients and p-values between the variables and the continuous variables in the first dimension of the PCA analysis.

The most important correlations in the first dimension are:

-   The negative correlation between the target variable "y" and the first dimension (-0.695), which means that this variable is strongly associated with the opposite direction of the first principal component.

-   The negative correlation between "duration" and the first dimension (-0.256), indicating that longer call durations are associated with the opposite direction of the first principal component.

-   The positive correlations between "euribor3m" (0.976), "emp.var.rate" (0.973), "cons.price.idx" (0.932), "cons.conf.idx" (0.925), and "nr.employed" (0.884) with the first dimension, which suggests that these economic indicators are associated with the same direction of the first principal component.

The small p-values (all are 0.000) indicate that these correlations are statistically significant. The age of the customer ("age_num") and the number of contacts performed during the current campaign ("campaign") also show some correlation with the first dimension, although to a lesser extent than the other variables.

```{r}
# Print the dimension description results
print(dimdesc_result$Dim.2)
# creates a variance plot that shows the contribution of each variable to each axis, and a contribution plot that shows the most contributing variables to each axis.
fviz_contrib(pca_result, choice = "var", axes = 2) +
  ggtitle("PCA: Contributions of Variables to PC2") +
  theme(text = element_text(size = 14)) +
  scale_shape_manual(values = c(16, 17, 15)) +
  scale_color_manual(values = c("#FFA500", "#008000", "#0000FF"))
```

This output shows the link between the variables and the continuous variables in terms of the R-square correlation and p-value. The most important correlations for the second dimension are:

-   Duration: this variable has the highest correlation with the second dimension, meaning that it contributes the most to the definition of this axis.

-   y: this variable also has a significant correlation with the second dimension, indicating that it is relevant for the analysis of this axis.

-   nr.employed and campaign: these variables have a moderate correlation with the second dimension, suggesting that they are also relevant to some extent.

-   emp.var.rate and euribor3m: these variables have a weaker correlation with the second dimension, but still contribute to its definition.

-   cons.price.idx, age_num, and cons.conf.idx: these variables have a negligible correlation with the second dimension, meaning that they do not play an important role in the analysis of this axis.

Overall, this output indicates that the second dimension is mostly defined by the duration variable, followed by other variables related to the campaign and employment, while other variables have a minor influence on this dimension.

So, in this case, we have run PCA on a dataset and extracted the first and second principal components, which explain 62% and 18.6% of the total variance, respectively.

The interpretation of the first dimension suggests a strong link between economic indicators, such as euribor3m, emp.var.rate, cons.price.idx, cons.conf.idx, and nr.employed, and the target variable y, as they show high positive correlation. Conversely, the variable duration shows negative correlation, which suggests that it has an inverse relationship with the economic indicators and y. In this sense, the first dimension can be interpreted as an economic indicator.

On the other hand, the second dimension is mainly driven by the variable duration and target variable y, which show high positive correlation, while the economic indicators have a much lower correlation. This suggests that duration and y are important features for distinguishing the data points along the second dimension, which could be interpreted as a behavioral indicator.

The first dimension appears to capture the economic aspects of the data,
while the second dimension reflects the behavioral aspect. Although the
first dimension explains more variance, the second dimension is also
important as it captures different aspects of the data that are not
captured by the first dimension. Therefore, considering both dimensions
is crucial for a comprehensive understanding of the dataset.

```{r}


```

```{r}


```

```{r}
# Representation of clouds
fviz_pca_var(pca_result, axes = c(1,2 ), col.var = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE, ggtheme = theme_classic())
=======
rang<-inds[order(inds$`Dim 2`, decreasing = TRUE),]
res.mca$ind$coord[row.names(rang)[1:10],2]
df[which(row.names(df) %in% row.names(res.mca$ind$coord
                                      [row.names(rang)[1:10],])),1:20]
>>>>>>> origin/pau
```

A la següent gràfica podrem veure sobre sobre el pla quins individus son els més contributius (marcats en vermell) i els menys (en groc).

```{r}
<<<<<<< HEAD
#c(vars_res, vars_dis,vars_con)
#ll <- which( df$mout == "YesMOut")
#res.pca<-PCA(df[,c(vars_res, vars_con)],quali.sup=c(1),quanti.sup= c(3), ind.sup = ll) 
#plot(res.pca, choix="ind",invisible=c("ind","ind.sup"), cex=0.7, graph.type = "classic")
# 
#lines(res.pca$quali.sup$coord[1:2,1],res.pca$quali.sup$coord[1:2,2],lwd=2,col="black") # Does not work unless graph.type "classic" is set

# Manually producing the plot
#plot(res.pca$ind$coord[,1],res.pca$ind$coord[,2],pch=19,col="grey30")
#points(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],pch=15,col="magenta")
# lines(res.pca$quali.sup$coord[7:10,1],res.pca$quali.sup$coord[7:10,2],lwd=2,lty=2,col="blue")
#text(res.pca$quali.sup$coord[,1],res.pca$quali.sup$coord[,2],labels=names(res.pca$quali.sup$coord[,1]),col="magenta",cex=0.8)

#res.pca$quali.sup$coord

=======
# A l'hora de fer les gràfiques per individus i categories,
# posarem com a invisible els individus suplementaris per no tenir-los en
# compte (individus amb outliers multivariants)
>>>>>>> origin/pau

fviz_mca_ind(
  res.mca,
  geom=c("point"),
  col.ind="contrib",
  invisible=c("ind.sup"),
  gradient.cols=c("yellow2", "red")
)
```

### 3. Interpreting map of categories

```{r}
fviz_mca_var(res.mca,
             choice="mca.cor",
             repel = T)
```

<<<<<<< HEAD
```{r}
prc<-pca_result$ind$coord[,1:3] # 3 components principals (kaiser)
dim(prc)
```
### Classification

```{r}
dist<-dist(prc)  # coordenates are real - Euclidean metric
kc<-kmeans(dist, 5, iter.max=30, trace=TRUE) #caclulate the distances, it turns into a matrix
```
We see from the output that in 4 iterations it has converged.
We now procceed to save in the data frame the number of clusters.
```{r}
df_numeric$claKM<-0
df_numeric$claKM<-kc$cluster
df_numeric$claKM<-factor(df_numeric$claKM)
barplot(table(df$claKM),col="darkslateblue",border="darkslateblue",main="[k-means]#observations/cluster")
```
```{r}
dim(df_numeric)
cat.res <-catdes(df_numeric,grep("^claKM$", colnames(df_numeric)))

```
The output shows the results of a cluster analysis based on the K-means algorithm. The analysis was carried out using eight quantitative variables and a categorical variable 'y' representing whether or not the client subscribed to a product.

The first table shows the Eta squared value and p-value for each variable in relation to the cluster variable. The Eta squared value indicates the proportion of variance explained by the cluster variable, and the p-value indicates the statistical significance of the relationship between the variables. All variables showed a significant relationship with the cluster variable except for age_num.

The second table provides a detailed description of each cluster based on the quantitative variables. The analysis resulted in five distinct clusters.

Cluster 1 is characterized by a high duration of calls, low number of campaigns, high consumer confidence, low consumer price index, low consumer confidence index, and low number of employees and employment variation rate. This group represents a small proportion of customers, but stands out for its high call duration, which may indicate a high-quality interaction between the customer and the company.

Cluster 2 is characterized by an extremely high call duration, high employment rate, high employment variation rate, high euribor value, positive employment variation rate, and high subscription rate to the product. This group is also an attractive target for advertising campaigns.

Cluster 3 is characterized by low call duration, low product subscription rate, high consumer confidence index, high consumer price index, and high number of employees. This group represents the majority of customers. The low call duration and low number of subscriptions may suggest that this group is less receptive to advertising campaigns, although their high consumer confidence index may be an opportunity for brand building and customer loyalty.

Cluster 4 is characterized by a high subscription rate to the product and low values of employment variation rate, consumer confidence index, consumer price index, and euribor. This group may represent loyal customers or customers with a high level of satisfaction with the product or service.

Cluster 5 is characterized by high subscription rates to the product, high employment rate, high call duration, high euribor value, high employment variation rate, and high consumer price index. This group may represent a high-value target for advertising campaigns due to their high subscription rate and favorable economic indicators.


### Hierarchical clustering

```{r}
# Perform hierarchical clustering on the first two principal components
#hclust_result <- hclust(dist(pca_result$ind$coord[, 1:2]))

# Visualize the results using a dendrogram
#fviz_dend(hclust_result, cex = 0.5, k = 2)

# Cut the dendrogram into three clusters
#cut_result <- cutree(hclust_result, k = w)

# Describe the clusters
#summary(cut_result)

res.hcpc <- HCPC(pca_result,nb.clust = -1, order = TRUE)
table(res.hcpc$data.clust$clust)
barplot(table(res.hcpc$data.clust$clust), col="darkslateblue", border="darkslateblue", main="[hierarchical] #observations/cluster")
```
```{r}
  # categorical variables which characterizes the clusters
```

```{r}
res.hcpc$desc.var$quanti.var    # description of each cluster by the categories
res.hcpc$desc.var$quanti
```

Per a relitzar la descripció dels grups d'individus, hem de realitzar una agrupació jeràrquica dels components principals (HCPC).

Agafem `r res.hcpc$call$t$nb.clust` clusters, ja que són els que ens indica el propi HCPC que hem d'incloure degut a la inèrcia acumulada d'aquests.

A la següent gràfica es pot veure les inèrcies per cada parella de clusters. Veiem que les més significatives són de la 1 a la 2 i en menor mesura la 3 (les que ens recomanava agafar el HCPC).

```{r}
#fviz_dend(res.hcpc, show_labels = FALSE)
plot(res.hcpc, choice = "bar")
```

A continuació imprimirem en un factor map tots els individus agrupats amb els diferents clusters que tenim. Podem veure com el cluster 1 esta completament difgerenciat de la resta, i el cluster 2 està disper i te punts desviats que provoquen que abarquin molta superfície sense individus.

```{r}
fviz_cluster(res.hcpc, geom = "point")
```

A continuació durem a terme la descripció de clusters envers les variables i categories més rellevants en ells.

Primer de tot veiem les variables més relacionades amb tots els clusters:

```{r}
res.hcpc$desc.var
```
The analysis is based on a cluster analysis and its description in terms of quantitative variables. In this case, eight variables have been used for the analysis. The table describes the link between the cluster variable and the quantitative variables, followed by the description of each cluster individually.

The variable "y" (customer subscription to the product) has a very strong relationship with the cluster variables in all three groups, with an Eta2 relationship of 98%, 98%, and 98% respectively. This indicates that customer subscription is an important factor in the separation of clusters.

Cluster 1 is described by a high level of call duration, a low number of campaigns, high consumer confidence, a low consumer price index, low consumer confidence index, and a low number of employees and employment rate variation. This group represents a small proportion of customers but stands out for its high call duration, which may indicate high-quality interaction between the customer and the company.

Cluster 2 is characterized by an extremely high call duration, a high employment rate, high employment rate variation, high euribor value, and positive employment rate variation. This group also has a high product subscription rate, indicating that this group is an attractive target for advertising campaigns.

Cluster 3 is described by low call duration, low product subscription rate, high consumer confidence index, high consumer price index, and high number of employees. This group represents the majority of customers. The low call duration and low number of subscriptions may suggest that this group is less receptive to advertising campaigns, although their high consumer confidence may be an opportunity for brand building and customer loyalty.

## Anàlisis CA

```{r}
# Convert the 'duration' variable to a factor with 7 levels
df$duration_cat <- cut(df$duration, breaks = 7, labels = FALSE)



# Perform Correspondence Analysis on the data
#ca_result <- CA(df_numeric, graph = FALSE, ncp = 2)

# Visualize the results using a biplot
#fviz_ca_biplot(ca_result, repel = TRUE)
```

```{r}
# Describe the results
#summary(ca_result)
```
=======
Podem veure que contribueixen en gran mesura les variables previous i poutcome per ambdues dimensions, mentres que per la dimensió 1 també contribuieixen month i contact. Education i job tenen una contribució en les dues dimensions en menor mesura de les mencionades anteriorment.

```{r}
fviz_mca_var(res.mca,
             alpha.var="contrib",
             repel = T)
```

Les categories més contributives en ambdues dimensions són "success" (categoria de "poutcome", variable que hem vist que contribuia en les dues dimensions) i "yes" (categoria de previous que també contribuia en les dues dimensions), la resta de categories no són tan determinants.
>>>>>>> origin/pau

Representarem les categories de les variables més representatives: poutcome, month i contact (com que poutcome i previous estan relacionades, sols imprimirem la més significativa, és a dir, poutcome).

```{r}
# A l'hora de fer les gràfiques per individus i categories, posarem
# com a invisible els individus suplementaris per no tenir-los en compte
# (individus amb outliers multivariants)
fviz_mca_ind(res.mca,
             label="none",
             invisible=c("ind.sup"),
             geom = c("point"),
             habillage="poutcome")

fviz_mca_ind(res.mca,
             label="none",
             invisible=c("ind.sup"),
             geom = c("point"),
             habillage="contact")

fviz_mca_ind(res.mca,
             label="none",
             invisible=c("ind.sup"),
             geom = c("point"),
             habillage="month")
```

En els tres gràfics anteriors, podem veure com les categories formen grups diferenciats, sobretot en les dues primeres. A la última gràfica les categories no estan tan marcades, tot i que es veu una tendència semblant entre categories.

### 4. Interpreting the axes association to factor map

Per auquest punt durem a terme una descripció de dimensions a través de la funció dimdesc per poder veure les variables i categories més relacionades amb cada dimensió. Realitzarem l'anàlisis amb profunditat de les tres primeres dimensions ja que són les més rellevants.

```{r}
res.des <- dimdesc(res.mca)
```

#### Dimensió 1

```{r}
res.des$`Dim 1`$quali
```

Les variables que més ens representan la primera dimensió són les variables següents:

-   contact (0.528)

-   month (0.488)

-   poutcome (0.384)

Aquestes tres variables són les que hem vist que estaven més relacionades anteriorment de forma gràfica.

```{r}
res.des$`Dim 1`$category
```

Les categories que més representen la primera dimensió són les següents:

-   success de poutcome (1.175)

-   Yes de previous (1.01)

-   apr de month (0.647)

Tot i que hi hagi contribucions negatives amb valors més destacats, no els tenim en compte ja que són categories contràries a les que tenim en positiu.

Aquestes tres categories són les que hem vist que estaven més relacionades anteriorment de forma gràfica.

#### Dimensió 2

```{r}
res.des$`Dim 2`$quali
```

Les variables que més ens representan la segona dimensió són les variables següents:

-   poutcome (0.6062)

-   previous (0.5940)

-   education (0.215)

Aquestes tres variables són les que hem vist que estaven més relacionades anteriorment de forma gràfica.

```{r}
res.des$`Dim 2`$category
```

Les categories que més representen la segona dimensió són les següents:

-   success de poutcome (1.734)

-   Yes de previous (1.226)

-   basic de education (0.318)

Aquestes tres categories són les que hem vist que estaven més relacionades anteriorment de forma gràfica.

#### Dimensió 3

```{r}
res.des$`Dim 3`$quali
```

Les variables que més ens representan la segona dimensió són les variables següents:

-   job (0.458)

-   education (0.427)

-   age (0.234)

```{r}
res.des$`Dim 3`$category
```

Les categories que més representen la segona dimensió són les següents:

-   Gran de age (1.289)

-   illiterate de education (1.068)

-   oct de month (-0.649)

### 5. Perform a MCA taking into account also supplementary variables

Realitzarem el nou anàlisis MCA amb les variables continues com a suplementàries. Per a realitzar el nou model obviarem la variable "age_num", ja que la tenim en compte a la variable "age" i ens alteraria els resultats incloure-la dues vegades.

```{r}
res.mca_sup<-MCA(df[,c(var_res, var_con[2:8], var_dis[1:11]) ], quali.sup=1,
                 quanti.sup = c(2:8), ind.sup=llmout, graph = F)
```

Igual que hem fet a l'apartat anterior, realitzarem una nova descripció de dimensions per veure les variacions.

```{r}
res.des_sup <- dimdesc(res.mca_sup)
```

```{r}
res.des_sup
```

Per cada dimensió podem veure les correlacions que hi ha amb les variables continues, la majoria d'aquestes són índex econòmics que contribueixen de forma negativa a les dimensions.

Tant per variables com per categories, el fet d'incloure les variables continues com a suplementàries no ha variat el seu resultat ni contribució.

## Clustering MCA

### Description of clusters

Per a relitzar la descripció dels grups d'individus, hem de realitzar una agrupació jeràrquica dels components principals (HCPC).

```{r}
# Posem nb.clust = -1 perquè utilitzi el numero de clusters que ens recomana
res.hcpc_mca<-HCPC(res.mca, nb.clust = -1, order=TRUE)
```

Agafem `r res.hcpc_mca$call$t$nb.clust` clusters, ja que són els que ens indica el propi HCPC que hem d'incloure degut a la inèrcia acumulada d'aquests.

A la següent gràfica es pot veure les inèrcies per cada parella de clusters. Veiem que les més significatives són de la 1 a la 4 (les que ens recomanava agafar el HCPC).

```{r}
plot(res.hcpc_mca, choice = "bar")
```

A continuació imprimirem en un factor map tots els individus agrupats amb els diferents clusters que tenim. Podem veure com el cluster 1, 3 i 4 estan completament difgerenciats, però el cluster 2 està dispers amb el primer i tercer. També observem com els clusters 3 i 4 tenen punts molt desviats que provoquen que abarquin molta superfície sense individus.

```{r}
fviz_cluster(res.hcpc_mca, geom = "point")
```

A continuació durem a terme la descripció de clusters envers les variables i categories més rellevants en ells.

Primer de tot veiem les variables més relacionades amb tots els clusters:

```{r}
res.hcpc_mca$desc.var$test.chi2
```

Les següents variables són les que ens aporten més informació per representar els clusters. Són totes variables discretes ja que es tracta d'un anàlisis MCA:

-   job
-   education
-   poutcome
-   month

Seguidament podem veure, per cadascun dels 4 clusters escollits, les categories que els conformen. Aquests valors els relacionarem amb les variables que hem vist que estan més relacionades per veure'n les seves categories exactes:

```{r}
res.hcpc_mca$desc.var$category
```

-   Cluster 1

    -   job

        1.  blue.collar (51,45%)

        2.  services (17,87%)

        3.  admin (11.92%) (de forma negativa)

    -   education

        1.  basic (66,56%)

        2.  high.school (31,7%)

    -   month

        1.  may (85,33%)

Com a informació addicional, comentar que cap dels individus dins del cluster ha estat contactat previament (previous=no), això provoca que hi hagi un 0% de poutcome=success.

-   Cluster 2

    -   job

        1.  technician (77.21%)

    -   education

        1.  professional.course (67.67%): Veiem una clara relació entre aquests nivells d'estudis i la categoria technician de la variable job: la majoria de fp estan destinades a feines tècniques.

        2.  university.degree (15.23%) (de forma negativa)

    -   month

        1.  aug (10,69%)
        
        2.  apr (4,53%) (de forma negativa)
        
        En aquest cluster la variable té molt poc pes.


-   Cluster 3

    -   job

        1.  admin (47,91%)

        2.  management (17,13%)

        3.  self-employed (12,03%)
        
        Aquestes tres categories estan bastant relacionades amb el tipus de feina que són, ja que feines administratives, de control i d'autònom són similars.
        
    -   education

        1.  university.degree (68.14%): Aquesta categoria esta bastant relacionada amb els nivells de job descrits anteriorment, ja que són posicions de feina altes i aquí es descriu el nivell més alt d'estudis registrat.

        2.  high.school (21.89%) (de forma negativa)
        
    -   month

        1.  may (44,01%) (de forma negativa)
        
        2.  apr (19,31%) 
        
        3.  jul (11,29%)
        
        4.  aug (8,31%)


-   Cluster 4

    -   month
    
        1.  may (53,49%) (de forma negativa)

        2.  apr (40,70%)

En aquest últim cluster, com que tenen un gran pes previous i poutcome, les variables job i education no són representatives. En el cas de previous, la categoria més significant és yes (100%) i de poutcome és success (96,51%). Destacar també que tots els poutcome=success es troben en aquest cluster.

-   Cut quality

La qualitat de la partició amb 4 clusters és del `r (res.hcpc_mca$call$t$within[1]- res.hcpc_mca$call$t$within[4])/res.hcpc_mca$call$t$within[1]*100`%.

### Parangons and class-specific individuals

En aquest apartat podem observar els individus més cercans i més allunyats dels centroides de cada cluster.

A la taula següent podem veure, per cada cluster, els 5 individus més cercans als centroides amb les respectives distàncies:

```{r}
res.hcpc_mca$desc.ind$para
```

A la taula següent podem veure, per cada cluster, els 5 individus més distants als centroides amb les respectives distàncies:

```{r}
res.hcpc_mca$desc.ind$dist
```


```{r}
res.hcpc_mca$data.clust[which(rownames(res.hcpc_mca$data.clust)%in%names
                              (res.hcpc_mca$desc.ind$para[[1]])),]
res.hcpc_mca$data.clust[which(rownames(res.hcpc_mca$data.clust)%in%names
                              (res.hcpc_mca$desc.ind$dist[[1]])),]

res.hcpc_mca$data.clust[which(rownames(res.hcpc_mca$data.clust)%in%names
                              (res.hcpc_mca$desc.ind$para[[2]])),]
res.hcpc_mca$data.clust[which(rownames(res.hcpc_mca$data.clust)%in%names
                              (res.hcpc_mca$desc.ind$dist[[2]])),]

res.hcpc_mca$data.clust[which(rownames(res.hcpc_mca$data.clust)%in%names
                              (res.hcpc_mca$desc.ind$para[[3]])),]
res.hcpc_mca$data.clust[which(rownames(res.hcpc_mca$data.clust)%in%names
                              (res.hcpc_mca$desc.ind$dist[[3]])),]

res.hcpc_mca$data.clust[which(rownames(res.hcpc_mca$data.clust)%in%names
                              (res.hcpc_mca$desc.ind$para[[4]])),]
res.hcpc_mca$data.clust[which(rownames(res.hcpc_mca$data.clust)%in%names
                              (res.hcpc_mca$desc.ind$dist[[4]])),]
```

En les taules anteriors hem pogut veure els valors de les variables que tenen els individus més propers i llunyans de cada cluster. D'aquí podem treure les següents conclusions:

-   Els individus més propers, tenen valors de les variables corresponents amb les categories vistes a la descripció de clusters, per tant, té sentit que siguin els individus més cercans al centroide.

-   Els individus més llunyans, tenen valors de les variables contraris amb les categories vistes a la descripció de clusters, per tant, té sentit que siguin els individus més distants al centroide.

## Comparison of clusters

### Target (y)


### Target (duration)
